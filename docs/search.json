[
  {
<<<<<<< HEAD
=======
    "objectID": "workshops/index.html",
    "href": "workshops/index.html",
    "title": "Intro to R: Data Wrangling",
    "section": "",
    "text": "Illustration from Hadley Wickham’s 2019 talk, The Joy of Functional Programming\n\n\n\n\n\nInstructional Documentation\n\n\nSource Code\n\n\n\n\nOverview\nThis workshop was developed for ES40: Critical Thinking and Evidence Based Reasoning at the University of California, Santa Barbara.\n\n\nAbstract\nR is a programming language that enables us to turn data into understanding. In this workshop, we’ll introduce the {dplyr} package of the {tidyverse}, which houses many functions that make data easier to work with, as an introduction to R and its applications for environmental data."
  },
  {
    "objectID": "posts/tfp_analysis/index.html",
    "href": "posts/tfp_analysis/index.html",
    "title": "Agriculture Total Factor Productivity Growth in Global Income Classes",
    "section": "",
    "text": "Introduction\nThe agricultural sector faces opposing pressures of sustaining a growing population while minimizing its unfavorable outcomes on finite environmental resources1. In an effort to simultaneously move towards these goals, countries around the world have prioritized agricultural productivity. One of the most informative measures of agricultural productivity is total factor productivity (TFP). TFP compares gross outputs of crop, animal and aquaculture products to inputs of land, labor, capital and material resources utilized in farm production2. As gross output increases at a faster rate than total inputs, total factor production improves, which eases tensions on environmental resources and food security, and boosts economic growth3. TFP is expressed generally by the equation: \\[TFP=Y/X\\] where Y represents gross output and X represents total inputs.\nTFP is an important measure for informing policy priorities for agricultural productivity. These policies include investments in research and development, incentivizing economic reforms for farmers, rural education and extension, and improvments in infrastructure4. Understanding the effects of individual inputs on TFP can direct decision making as it relates to resource allocation for these policy investments.\n\n\n\n\n\nThis analysis will regress global TFP indices on inputs of land, labor, capital, and materials to examine the effects of these inputs on gross outputs. This regression can be utilized to maximize TFP growth rates by differentiating efficiency levels of individual inputs as they relate to gross productivity, which can direct resource allocation to technological improvements of inefficient input systems.\nAdditionally, it will examine TFP growth rates for country groupings of income class (defined by the World Bank) by testing for mean differences. It will also forecast TFP growth rates for years 2020-2030 at a global scale and for income classes by employing automated autoregressive moving average (ARIMA) models. Understanding nuances in TFP growth for varying income scales can be useful in further research to refine regressions that direct policy prioritization.\nAll relevant analysis outputs are included in the Analysis section - for detailed code concerning model checking, reference the Model Testing and Supporting Figures section.\n\n\nData\nData used in this analysis is sourced from the United States Department of Agriculture’s (USDA) Economic Research Services5. This data is publicly available here. Data files contain annual indices for agricultural TFP, outputs, and inputs for individual countries, major global regions, and countries grouped by income levels for years 1961-2020. Detailed data on land, labor, capital and material inputs used to construct TFP indices is also included, but are not contained in the subsetted data used for the purposes of this particular analysis. TFPs are indexed with a base year of 2015 such that TFP values for countries and regions are set to 100 in 2015.\nIt is relevant to note that TFP index comparison between geographical regions provides information regarding TFP growth rates, but is not informative for direct comparison of productivity levels.\n\n\nsee code\n#country plot data \ntfp_country_p <- tfp_all |> \n  filter(level == 'Country',\n         income %in% c('HI', 'MI-U', 'MI-L', 'LI')) |> \n  select(year, tfp_index, income) |> \n  group_by(income, year) |> \n  summarize_all(mean, na.rm = TRUE)\n\n#country income factor levels\ntfp_country_p$income <- factor(tfp_country_p$income, levels = c('HI', 'MI-U', 'MI-L', 'LI'))\n\n#income class TFP plot\ntfp_country_plot <- ggplot(data=tfp_country_p) +\n  geom_line(aes(x=year, y=tfp_index, group=income, color=income), alpha = 0.5) + \n  theme_minimal() +\n  labs(y = 'TFP Index', x = 'Year') +\n  scale_x_discrete(breaks = scales::pretty_breaks(n=10)) +\n  scale_color_discrete(name = 'Income Class') +\n  theme(plot.title = element_text(hjust = 0.5))\n\n#global TFP plot\ntfp_world_plot <- ggplot(data = tfp_world) + \n  geom_line(aes(x=year, y=tfp_index, group = 1)) + \n  labs(y = 'TFP Index') +\n  theme_minimal()  +\n  scale_x_discrete(breaks = scales::pretty_breaks(n=10)) \n\n#income class + global TFP plot\nsubplot(tfp_world_plot, tfp_country_plot, nrows = 2, shareX = TRUE, shareY= TRUE) |> \n          layout(title = list(text = 'Fig 1. Total Factor Production Indices (1961-2020)', font = list(size = 12)), xaxis = list(x = 0.5,  \n    y = 0,  \n    text = \"Income Class Grouped\",  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    xanchor = \"center\",  \n    yanchor = \"bottom\",  \n    showarrow = FALSE),\n                 annotations = list(list(x = 0.03,  \n    y = 0.95,  \n    text = \"Global\",  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    xanchor = \"center\",  \n    yanchor = \"bottom\",  \n    showarrow = FALSE) , list(x = 0.13,  \n    y = 0.4,  \n    text = \"Income Class Grouped\",  \n    xref = \"paper\",  \n    yref = \"paper\",  \n    xanchor = \"center\",  \n    yanchor = \"bottom\",  \n    showarrow = FALSE)))\n\n\n\n\n\n\n\n\nAnalysis\n\nMultiple Linear Regression\nA stepwise regression is performed to compare a linear model containing no predictors to a full linear model containing all input variables (land, labor, capital, and materials). The results of this regression suggest that materials are not a relevant predictor in estimating TFP. Consequently, we opt for a model containing three predictor variables: land, labor, capital.\nFull model: \\[TFP = \\beta_0 + \\beta_1*land_i + \\beta_2*labor_i + \\beta_3*capital_i + \\beta_4 * materials_i + \\varepsilon_i\\]\nReduced model: \\[TFP = \\beta_0 + \\beta_1*land_i + \\beta_2*labor_i + \\beta_3*capital_i  + \\varepsilon_i\\]\nThe r-squared, adjusted r-squared, and Mallow’s Cp values of the leaps procedure affirm that this model is optimal in predictive accuracy. A pairwise analysis of the reduced predictor variables suggests that there may be significant variable interactions. A second stepwise regression is performed with a full linear model containing all interactions between refined predictor variables, which suggests that there is a significant interaction between land and capital inputs. Therefore, we include this interaction term in the refined model.\nFull model: \\[TFP = \\beta_0 + \\beta_1*land_i + \\beta_2*labor_i + \\beta_3*capital_i  + \\beta_4*land_i:capital_i + \\\\ \\beta_5*land_i:labor_i + \\beta_6*land_i:capital_i + \\varepsilon_i\\]\nReduced model: \\[TFP = \\beta_0 + \\beta_1*land_i + \\beta_2*labor_i + \\beta_3*capital_i + \\beta_4 * land_i:capital_i + \\varepsilon_i\\]\nThe Normal Q-Q plot of the residuals evidences slight non-normality. A log transformation of the response variable, TFP, is performed for normalization.\nA summary of the updated model is evaluated. A Residuals vs. Fitted plot displays a roughly even spread of residuals around the zero line, which suggests that equal variance and linearity assumptions are satisfied. A Normal Q-Q plot suggests normality given its relative linearity. Therefore, we accept the reduced and transformed model as a final model.\nAccepted model: \\[log(TFP) = \\beta_0 + \\beta_1*land_i + \\beta_2*labor_i + \\beta_3*capital_i + \\beta_4 * land_i:capital_i + \\varepsilon_i\\]\n\n\nsee code\n#multiple regression model\nmodel2 <- lm(log(y) ~ x1+x2+x3+x2:x1)\n\n#summary table for linear regression\ntab_model(model2, \n          pred.labels = c('Intercept', 'Land',\n                          'Labor', 'Capital', 'Land * Capital'),\n          dv.labels = 'Log Total Factor Production',\n          string.ci = '95% Conf. Interval',\n          string.p = 'P-value',\n          title = 'Tbl 1. Transformed Linear Model Results for TFP Regression',\n          digits = 7)\n\n\n\n\nTbl 1. Transformed Linear Model Results for TFP Regression\n\n \nLog Total Factor Production\n\n\nPredictors\nEstimates\n95% Conf. Interval\nP-value\n\n\nIntercept\n4.5154529\n4.4889663 – 4.5419395\n<0.001\n\n\nLand\n0.0008662\n0.0005873 – 0.0011451\n<0.001\n\n\nLabor\n-0.0019178\n-0.0021221 – -0.0017135\n<0.001\n\n\nCapital\n-0.0000779\n-0.0001752 – 0.0000194\n0.117\n\n\nLand * Capital\n0.0000062\n0.0000047 – 0.0000077\n<0.001\n\n\nObservations\n10187\n\n\nR2 / R2 adjusted\n0.075 / 0.075\n\n\n\n\n\n\nGiven coefficient estimates of our performed regression, the final model is represented as: \\[log(TFP) = 4.515429 + 0.0008662*land_i - 0.0019178*labor_i - 0.0000779*capital_i + \\\\ 0.0000062 * land_i:capital_i + \\varepsilon_i\\] The model regression indicates that capital is not a significant predictor variable (p = 0.117); however, all other predictor variables are found to be significant at the 0.001 level (p < 0.001 for all remaining variables). Given that the interaction between land and capital variables is significant (p < 0.001), we opt to preserve capital as a predictor variable in spite of its insignificant predictive power. It is relevant to note that the model summary values are based off of a log-transformed model - for interpretability of these results, it is recommended an inverse transformation be performed on the model estimates. In spite of this, given the signs of the estimates, it can be noted that labor and capital inputs are negatively correlated with TFP, while land and land/capital interaction inputs are positively correlated with TFP. The overall predictive power of the model is low (evidenced by adjusted r-squared = 0.075), which suggests that it is not well equipped to accurately predict TFP variability.\n\n\nDifferences in Means for Varying World Economies\nWe move to performing statistical tests to compare means of TFP indices between countries grouped by income class: low income (LI), lower-middle income (MI-L), upper-middle income (MI-U), high income (HI). Our first step is to visualize the distribution of the data for all income classes.\n\n\nsee code\n#income class factors\ntfp_income <- tfp_country |> \n  select(income, tfp_index) |>\n  filter(income %in% c('LI','MI-U','HI','MI-L')) |> \n  mutate(income = as.factor(income))\n\n#distribution visualization for income class factor data\nviolin_plot <- ggviolin(tfp_income, x='income', y='tfp_index', fill = 'income', \n         order = c('LI', 'MI-L', 'MI-U', 'HI'), \n         ylab = 'TFP Index', xlab = 'Income Class',\n         draw_quantiles = 0.5, add = 'boxplot') \nggpar(violin_plot, legend.title = 'Income Class', xlab = '',\n      caption = 'Fig 2. Distribution of TFP Indices for Varying Income Classes',\n      ggtheme = theme_minimal())\n\n\n\n\n\nGiven that there are significant outliers in each income class, mean differences are tested using Kruskal-Wallis and Dunn tests, both non-parametric methods that have no assumptions of normality.\nThe null hypothesis: \\[H_0: \\mu_{low}=\\mu_{mid-low} = \\mu_{mid-high} = \\mu_{high}\\]\nThe alternative hypothesis:\nHA: mean TFP indices are not equal across all income classes\n\n\nsee code\n#kruskal test for difference in means\nincome_k <- kruskal_test(tfp_index ~ income, data = tfp_income)\n  \n#dunn test for difference in means \nincome_pairs_d <- dunn_test(tfp_index ~ income, data = tfp_income, p.adjust.method = 'bonferroni')\n\n#vizualize difference in means \nincome_pairs_d <- income_pairs_d |> \n  add_xy_position(x = 'income') \nggboxplot(tfp_income, x = 'income', y = 'tfp_index',\n          order = c('LI', 'MI-L', 'MI-U', 'HI')) +\n  stat_pvalue_manual(income_pairs_d, hide.ns = TRUE) +\n  labs(title = get_test_label(income_k, detailed = TRUE),\nsubtitle = get_pwc_label(income_pairs_d),\ncaption = 'Fig 3. Mean difference testing \\n for TFP in varying income classes',\ny = 'TFP Index', x = 'Income Level') +\n  theme()\n\n\n\n\n\nThe Kruskal-Wallis test (p<0.0001) rejects the null hypothesis; there is significant evidence to suggest that there is a difference in mean TFP indices between differing income classes. The Dunn test concludes that this difference is statistically significant between all income class pairings, indicated by significance asterisks of the above. We use this information primarily to validate the productivity of forecasting TFP growth at income class scales as opposed to globally, rather than for the purpose of directly comparing mean differences, since dataset indexing invalidates the capacity for direct productivity comparisons between groups.\n\n\nTFP Growth Forecasting\nWe employ autoregressive integrated moving average (ARIMA) models to predict global and income class grouped TFP indices until year 2030.\n\n\nsee code\n#add dates to world data\ntfp_time <- tfp_world |> \n  mutate(date = paste0(year, '-01-01'),\n         date = as.Date(date, format = '%Y-%m-%d')) |> \n  group_by(date) |> \n  summarize(tfp_index_mean = mean(tfp_index))\n\n#convert data to time series \nts <- xts(tfp_time$tfp_index_mean, tfp_time$date)\n\n#fit ts to automated ARIMA model\nfit <- auto.arima(ts)\n\n#visualize global forecasts \npred.tr <- predict(fit, n.ahead=10)\nU.tr <- pred.tr$pred + 2*pred.tr$se\nL.tr <- pred.tr$pred - 2*pred.tr$se\nts.plot(ts, xlim=c(0,length(ts)+12), ylim=c(min(ts),max(ts)+20))\ntitle(main = 'Forecasted TFP Indices for Global Data', sub = 'Fig 4. Forecasts for ARIMA(2,2,1) Model with 95% Confidence Intervals', cex.sub = 0.65)\nlegend('topleft', inset = 0.02,\n       legend = c('Forecast', '95% CI'),\n       col = c('red', 'blue'),\n       lty = c(1,2))\nlines(U.tr, col='blue', lty='dashed')\nlines(L.tr, col='blue', lty='dashed')\nlines((length(ts)+1):(length(ts)+10), pred.tr$pred, col='red')\n\n\n\n\n\nBased on historical trend, the forecast anticipates that global TFP will continue to grow in the coming decade. We look to forecasted values for more detailed information concerning this growth.\n\n\nsee code\n#income class forecasts \npred.hi <- predict(fit_hi, n.ahead=10)\npred.li <- predict(fit_li, n.ahead=10)\npred.mi <- predict(fit_mi, n.ahead=10)\npred.mil <- predict(fit_mil, n.ahead=10)\nyear <- c(2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030)\n\n#forecast data frame \npredictions <- data.frame(year, pred.li$pred, pred.mil$pred, pred.mi$pred, pred.hi$pred, pred.tr$pred) \ncolnames(predictions) <- c('Year', 'Low Income', 'Lower-Middle Income', 'Upper-Middle Income', 'High Income', 'World')\npredictions |> \n  kbl(caption = 'Tbl 2. Predicted TFP Indices') |> \n  kable_styling(bootstrap_options = 'striped', full_width = F)\n\n\n\n\nTbl 2. Predicted TFP Indices\n \n  \n    Year \n    Low Income \n    Lower-Middle Income \n    Upper-Middle Income \n    High Income \n    World \n  \n \n\n  \n    2021 \n    101.8986 \n    103.8871 \n    102.379 \n    105.5007 \n    105.6913 \n  \n  \n    2022 \n    101.8986 \n    104.3529 \n    102.379 \n    106.4207 \n    106.5413 \n  \n  \n    2023 \n    101.8986 \n    104.9014 \n    102.379 \n    107.3407 \n    107.2756 \n  \n  \n    2024 \n    101.8986 \n    105.5635 \n    102.379 \n    108.2607 \n    108.2272 \n  \n  \n    2025 \n    101.8986 \n    106.1743 \n    102.379 \n    109.1807 \n    109.1500 \n  \n  \n    2026 \n    101.8986 \n    106.7559 \n    102.379 \n    110.1007 \n    109.9950 \n  \n  \n    2027 \n    101.8986 \n    107.3610 \n    102.379 \n    111.0207 \n    110.8787 \n  \n  \n    2028 \n    101.8986 \n    107.9716 \n    102.379 \n    111.9407 \n    111.7803 \n  \n  \n    2029 \n    101.8986 \n    108.5730 \n    102.379 \n    112.8607 \n    112.6601 \n  \n  \n    2030 \n    101.8986 \n    109.1743 \n    102.379 \n    113.7807 \n    113.5402 \n  \n\n\n\n\n\nThe automated models for income class groups predict that low income and upper-middle income class countries will see no change in TFP indices throughout the forecasted decade. The models also predict that lower-middle income and high income countries will experience steady growth in TFP indices, which is reflected similarly in predicted consistent TFP growth for non-grouped world data.\n\n\n\nDiscussion\nThe maximization of agricultural total factor productivity will promote sustainable economic growth and can be used as a tool to ease the environmental burden of agriculture, so understanding the effects of inputs in TFP is pertinent to resource allocation in policy drivers.\nResults of the linear regression evidence that only land, labor, and land/capital interaction inputs are significant predictors of TFP, although we preserve capital inputs in our model due to its significance in interaction terms. The global model’s coefficient estimates indicate that land and land/capital interaction inputs are positively correlated with TFP - that is, growth rates of these inputs correlate to comparatively larger growth rates of gross outputs. Inversely, labor and capital inputs are negatively correlated with TFP - while increased inputs of these variables may increase total outputs, the growth rate of associated outputs is smaller than than the growth rate of its inputs.\nFor quantitative interpretations, we would transform summary coefficients following the equation \\[(\\exp(coefficient.estimate)-1)*100\\] such that they reflect percent increases (as opposed to unit increases) of the response variable, TFP, for one-unit increases in associated input variables.\nMean testing indicates that there is a statistically significant difference in mean TFP values between all income class pairs. A one-sided test of means may offer more insight regarding income class TFP distinctions.\nThe time series forecasting of TFP indices exhibits predicted increases in global TFP indices in the next decade, yet only lower-middle income and high income classifications are expected to see growing TFP, while low income and upper-middle income classifications are predicted to stagnate. As it pertains to low-income countries, it is reasonable to assume that there is limited flexibility for expenditure on the research and development that TFP growth necessitates, so this stagnation is largely unsurprising. A potential explanation for upper-middle income TFP stagnation is the “middle-income trap”6. However, these theories are largely assumptive. Cross-country differences in TFP growth within income classes is highly variable and depend on numerous factors including research and development, enabling environments, and economically disruptive shocks7, so forecasting measures are expected to be more informative and accurate for predictive models fit to a particular country of interest.\n\n\nFurther Research\nUnder the produced linear model, only 7.5% of variation in global TFP is explained by the model’s input variables (Tbl 1). It is reasonable to expect that a global model utilizing detailed land, labor, capital, and materials variables may perform better than a model comprised of only their summative indexes. A best fitting model to inform policy prioritization will be tailored to a particular country of interest, which eliminates the variability of cross-country differences in TFP.\nFurthermore, some of the residuals of the automated predictive models appear to be significant beyond the randomness of white noise (see Model Checking and Supporting Figures), which implies potential to produce more accurate forecasting models for income class groupings.\nFinally, current measures of TFP do not factor environmental impacts. Growing productivity rates ease natural resource demand in agriculture, which lessens pressure on finite environmental reserves; however, higher productivity can also result in higher externalities that negatively impact the environment. The Network on Agricultural TFP and the Environment, launched in 2017, aims to develop a framework for cross-country agricultural TFP comparisons to help address this issue8.\n\n\nConclusion\nThis analysis lends itself to understanding how policy priorities can be shaped to maximize agricultural TFP growth through individual inputs, and examines the differences of this index across various levels of income class.\n\n\nModel Testing and Supporting Figures\nFull analysis code is available here. Model testing and supporting figures contains various steps of regression model fitting and checking, as well as model fitting and checking for income class grouped ARIMA models.\n\n\nsee code\n#regression model variable assignments \ny <- tfp_country$tfp_index\nx1 <- tfp_country$land_index\nx2 <- tfp_country$labor_index\nx3 <- tfp_country$capital_index\nx4 <- tfp_country$materials_index\n\n#lower model\nmod0 <- lm(y~1)\n#upper model\nmod_upper <- lm(y~x1+x2+x3+x4)\n#stepwise regression for model selection \n#step(mod0, scope=list(lower = mod0, upper=mod_upper))\n\n\n\n\nsee code\n#r-squared, adjusted r-squared, and Mallow's Cp values of reduced model \nmod <- regsubsets(cbind(x1,x2,x3),y)\nsummary_mod <- summary(mod)\n#Mallow's Cp\nsummary_mod$which\n\n\n  (Intercept)    x1   x2    x3\n1        TRUE FALSE TRUE FALSE\n2        TRUE  TRUE TRUE FALSE\n3        TRUE  TRUE TRUE  TRUE\n\n\nsee code\n#r-squared values\nsummary_mod$rsq\n\n\n[1] 0.03808702 0.04533486 0.04966514\n\n\nsee code\n#adjusted r-squared values\nsummary_mod$adjr2\n\n\n[1] 0.03799258 0.04514738 0.04938516\n\n\nsee code\n#pairwise analysis of reduced variables\npairs(cbind(x1,x2,x3), main = 'Pairwise Analysis of Reduced TFP Regression Variables', labels = c('Land', 'Labor', 'Capital'))\n\n\n\n\n\nsee code\n#upper model with interaction terms \nmod_upper1 <- lm(y~x1+x2+x3+x1*x2+x1*x3+x1*x3)\n#stepwise regression for model selection \n#step(mod0, scope=list(lower = mod0, upper=mod_upper1))\nmodel1 <- lm(y~x1+x2+x3+x2:x1)\n\n\n\n\nsee code\n#fitted vs. residuals checking for non-transformed model \nplot(fitted(model1),residuals(model1))\nabline(h=0)\ntitle(main = 'Fitted vs. Residuals for non-transformed linear model')\n\n\n\n\n\nsee code\n#normal qqplot for non-transformed model\nqqnorm(residuals(model1))\nqqline(residuals(model1))\n\n\n\n\n\nsee code\n#histogram of non-transformed response variable (TFP) distribution\nhist(y, main = 'Distribution of non-transformed TFP response variable', xlab = 'TFP')\n\n\n\n\n\n\n\nsee code\n#log transformed model \nmodel2 <- lm(log(y) ~ x1+x2+x3+x2:x1)\n\n#fitted vs. residuals checking for log transformed model \nplot(fitted(model2),residuals(model2))\nabline(h=0)\ntitle(main = 'Fitted vs. Residuals for log-transformed model')\n\n\n\n\n\nsee code\n#normal qqplot for log transformed model\nqqnorm(residuals(model2))\nqqline(residuals(model2))\n\n\n\n\n\nsee code\n#histogram of log transformed response variable (TFP) distribution\nhist(log(y), main = 'Distribution of log transformed TFP', xlab = 'log(TFP)')\n\n\n\n\n\n\n\nsee code\n#low income forecasting \ntfp_time_li <- tfp_country |> \n  filter(income == 'LI') |> \n  mutate(date = paste0(year, '-01-01'),\n         date = as.Date(date, format = '%Y-%m-%d')) |> \n  group_by(date) |> \n  summarize(tfp_index_mean = mean(tfp_index))\n\nts_li <- xts(tfp_time_li$tfp_index_mean, tfp_time_li$date)\n\nfit_li <- auto.arima(ts_li)\n\n#checking residuals \ncheckresiduals(fit_li)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)\nQ* = 24.192, df = 10, p-value = 0.007108\n\nModel df: 0.   Total lags used: 10\n\n\n\n\nsee code\n#lower-middle income forecasting \ntfp_time_mil <- tfp_country |> \n  filter(income == 'MI-L') |> \n  mutate(date = paste0(year, '-01-01'),\n         date = as.Date(date, format = '%Y-%m-%d')) |> \n  group_by(date) |> \n  summarize(tfp_index_mean = mean(tfp_index))\n\nts_mil <- xts(tfp_time_mil$tfp_index_mean, tfp_time_mil$date)\n\nfit_mil <- auto.arima(ts_mil)\n\n#checking residuals \ncheckresiduals(fit_mil)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(2,2,1)\nQ* = 14.191, df = 7, p-value = 0.04788\n\nModel df: 3.   Total lags used: 10\n\n\n\n\nsee code\n#upper-middle income forecasting \ntfp_time_mi <- tfp_country |> \n  filter(income == 'MI-U') |> \n  mutate(date = paste0(year, '-01-01'),\n         date = as.Date(date, format = '%Y-%m-%d')) |> \n  group_by(date) |> \n  summarize(tfp_index_mean = mean(tfp_index))\n\nts_mi <- xts(tfp_time_mi$tfp_index_mean, tfp_time_mi$date)\n\nfit_mi <- auto.arima(ts_mi)\n\n#checking residuals \ncheckresiduals(fit_mi)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,1)\nQ* = 5.3068, df = 9, p-value = 0.8068\n\nModel df: 1.   Total lags used: 10\n\n\n\n\nsee code\n#high income forecasting \ntfp_time_hi <- tfp_country |> \n  filter(income == 'HI') |> \n  mutate(date = paste0(year, '-01-01'),\n         date = as.Date(date, format = '%Y-%m-%d')) |> \n  group_by(date) |> \n  summarize(tfp_index_mean = mean(tfp_index))\n\nts_hi <- xts(tfp_time_hi$tfp_index_mean, tfp_time_hi$date)\n\nfit_hi <- auto.arima(ts_hi)\n\n#checking residuals \ncheckresiduals(fit_hi)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,2,2)\nQ* = 4.4176, df = 8, p-value = 0.8176\n\nModel df: 2.   Total lags used: 10\n\n\n\n\n\n\n\nFootnotes\n\n\nNetwork on Agricultural Total Factor Productivity and the Environment. Oecd.org. Accessed December 4, 2022. https://www.oecd.org/agriculture/topics/network-agricultural-productivity-and-environment/↩︎\nFuglie K, Jelliffe J, Morgan S. Documentation and methods. Usda.gov. Published October 7, 2022. Accessed December 4, 2022. https://www.ers.usda.gov/data-products/international-agricultural-productivity/documentation-and-methods/↩︎\nFuglie K, Jelliffe J, Morgan S. Documentation and methods. Usda.gov. Published October 7, 2022. Accessed December 4, 2022. https://www.ers.usda.gov/data-products/international-agricultural-productivity/documentation-and-methods/↩︎\nFuglie K, Wang SL. New Evidence Points to Robust But Uneven Productivity Growth in Global Agriculture. Usda.gov. Published September 20, 2012. Accessed December 4, 2022. https://www.ers.usda.gov/amber-waves/2012/september/global-agriculture/↩︎\nFuglie K, Jelliffe J, Morgan S. International agricultural productivity. Usda.gov. Published October 7, 2022. Accessed December 4, 2022. https://www.ers.usda.gov/data-products/international-agricultural-productivity/↩︎\nGriffith B. Middle-Income Trap. In: Frontiers in Development Policy. The World Bank; 2011:39-43.↩︎\nFuglie K, Wang SL. New Evidence Points to Robust But Uneven Productivity Growth in Global Agriculture. Usda.gov. Published September 20, 2012. Accessed December 4, 2022. https://www.ers.usda.gov/amber-waves/2012/september/global-agriculture/↩︎\nNetwork on Agricultural Total Factor Productivity and the Environment. Oecd.org. Accessed December 4, 2022. https://www.oecd.org/agriculture/topics/network-agricultural-productivity-and-environment/↩︎"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Intro to R: Data Wrangling\n\n\na code-along workshop to learn basic examples of using quantitative data in environmental data science with the {tidyverse}\n\n\n\nGabrielle Smith\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
>>>>>>> parent of 93be2ae (finalizing eel modeling)
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gabrielle Smith",
    "section": "",
    "text": "I’m Gabrielle, and I am a recent graduate of the Bren School of Environmental Science & Management at the University of California, Santa Barbara with a Master’s in Environmental Data Science. I also earned a Statistics and Data Science B.S. from the University of California, Santa Barbara in 2022.\nI’m passionate about leveraging remote sensing techniques to address our environmental challenges and climate issues. If you’re curious about some of the projects I’ve worked on, you came to the right place!"
  },
  {
<<<<<<< HEAD
=======
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I grew up in San Diego, California. I moved to Santa Barbara in 2018 to pursue a Bachelor’s of Science in Statistics and Data Science. During that time, I experimented, a lot, with different educational enthusiasms.\nThroughout my life, I have always had a passion for people and the world around us. This passion turned into a career aspiration after a transformative volunteer trip to Thailand with GIVE Volunteers in 2019.\n\n\n\n\n\n\n\n\nA couple of friends that I made during my time in Thailand!\n\nDuring my two week experience abroad, I had the opportunity to establish a functional permaculture agricultural system in a remote village in Mueang Kong. For context, permaculture is a holistic design philosophy that promotes sustainable and regenerative farming practices. It strives to maximize productivity while minimizing negative environmental impacts by emulating natural patterns and processes. To learn more about permaculture in agriculture, you can find additional information here.\n\n\n\n\n\n\n\nI was fascinated by this pursuit of harmony between people and the natural world, and it has since become the driving force behind both my personal and professional goals. Inspired by this experience, I decided to pursue a Master’s degree in Environmental Data Science at the University of California, Santa Barbara. This choice allowed me to merge my educational background with my aspiration to contribute to systems that promote the well-being of both people and the planet.\nSince then, I have honed a diverse skill set in data science, equipping me with the tools to employ data-driven decision making in addressing environmental challenges in a sustainable and equitable way. My aim is to apply these skills in future career endeavors, working towards a better world that cares for the health of all living beings."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Portfolio",
    "section": "",
    "text": "Agriculture Total Factor Productivity Growth in Global Income Classes\n\n\nA statistical analysis of agriculture total factor productivity indices on global and income class scales, employing methods of multiple linear regression, mean testing, and…\n\n\n\nGabrielle Smith\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEel Species Distribution Modeling Using Boosted Trees\n\n\nA case study of species distribution modeling, following a modeling project described by Edith et al. 2008.\n\n\n\nGabrielle Smith\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#graduate",
    "href": "index.html#graduate",
    "title": "Gabrielle Smith",
    "section": "Graduate",
    "text": "Graduate\nUniversity of California, Santa Barbara | Santa Barbara, CA (June 2023)\nMaster of Environmental Data Science\n\nUndergraduate\nUniversity of California, Santa Barbara | Santa Barbara, CA (March 2022)\nBachelor of Science in Statistics and Data Science"
  },
  {
>>>>>>> parent of 93be2ae (finalizing eel modeling)
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Gabrielle Smith",
    "section": "Education",
    "text": "Education\n\nGraduate\nUniversity of California, Santa Barbara | Santa Barbara, CA (June 2023)\nMaster of Environmental Data Science\n\n\nUndergraduate\nUniversity of California, Santa Barbara | Santa Barbara, CA (March 2022)\nBachelor of Science in Statistics and Data Science"
  },
  {
<<<<<<< HEAD
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Gabrielle Smith",
    "section": "",
    "text": "I’m Gabrielle, and I am a recent graduate of the Bren School of Environmental Science & Management at the University of California, Santa Barbara with a Master’s in Environmental Data Science. I also earned a Statistics and Data Science B.S. from the University of California, Santa Barbara in 2022.\nI’m passionate about leveraging remote sensing techniques to address our environmental challenges and climate issues. If you’re curious about some of the projects I’ve worked on, you came to the right place!"
=======
    "objectID": "posts/boosted_eels/index.html",
    "href": "posts/boosted_eels/index.html",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "",
    "text": "In this post, I perform a case study example of species distribution modeling in a reproduction of the work done by Edith et al. 2008 [1]. For this case study, we model the short-finned eel (Anguilla australis) species using Boosted Regression Trees. This analysis is derived from an assignment for EDS232: Machine Learning in Environmental Science, as part of the curriculum for UCSB’s Master’s of Environmental Data Science program.\nBoosted regression trees, also known as gradient boosting machines, are machine learning techniques that combine concepts of regression trees (relating responses to predictors using recursive binary splits) and iterative boosting (an adaptive method for combining simple models to improve predictive performance). In this case study, decision trees are built sequentially using extreme gradient boosting to classify the presence or absence of short-finned eel in a given location with covariates including temperature, slope, rainy days, etc. The work provided by Edith et al. utilizes the {gbm} package in R, while this analysis follows a {tidymodels} approach in R."
  },
  {
    "objectID": "posts/boosted_eels/index.html#data",
    "href": "posts/boosted_eels/index.html#data",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Data",
    "text": "Data\nOur data for this analysis, “eel.model.data.csv” were retrived from the supplemental information of Edith et al. 2008. These data include the following variables:\n\n\n\nFigure 1: Table 1. from Elith et al. 2008 displaying the variables included in the analysis.\n\n\n\n\ncode\n# load required libraries\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(glmnet)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(sjPlot)\nlibrary(pROC)\nlibrary(RColorBrewer)\n\n\neel_data <- eel_data_raw |> \n  janitor::clean_names() \n\n# remove site number from modeling data\neel_model <- eel_data[,-1]\n\n# convert presence of angaus, downstream obstructions, and fishing methods to factors\neel_model$angaus <- as.factor(eel_model$angaus)\neel_model$ds_dam <- as.factor(eel_model$ds_dam)\neel_model$method <- as.factor(eel_model$method)\n\ntab_df(eel_data[1:5,],\n       title = \"Table. 1\")\n\n\n\n\nTable. 1\n\nsite\nangaus\nseg_sum_t\nseg_t_seas\nseg_low_flow\nds_dist\nds_max_slope\nus_avg_t\nus_rain_days\nus_slope\nus_native\nds_dam\nmethod\nloc_sed\n\n\n1\n0\n16.00\n-0.10\n1.04\n50.20\n0.57\n0.09\n2.47\n9.80\n0.81\n0\nelectric\n4.80\n\n\n2\n1\n18.70\n1.51\n1.00\n132.53\n1.15\n0.20\n1.15\n8.30\n0.34\n0\nelectric\n2.00\n\n\n3\n0\n18.30\n0.37\n1.00\n107.44\n0.57\n0.49\n0.85\n0.40\n0.00\n0\nspo\n1.00\n\n\n4\n0\n16.70\n-3.80\n1.00\n166.82\n1.72\n0.90\n0.21\n0.40\n0.22\n1\nelectric\n4.00\n\n\n5\n1\n17.20\n0.33\n1.00\n3.95\n1.15\n-1.20\n1.98\n21.90\n0.96\n0\nelectric\n4.70"
  },
  {
    "objectID": "posts/boosted_eels/index.html#split-and-resample",
    "href": "posts/boosted_eels/index.html#split-and-resample",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Split and Resample",
    "text": "Split and Resample\nWe split the above data into a training and test set, stratifying by the outcome score (angaus) to maintain class balance, improve generalization, and ensure the reliability of evaluation in modeling performances. We then use a 10-fold cross validation to resample the training set, also stratified by outcome score.\n\n\ncode\n# set a seed for reproducibility\nset.seed(123)\n\n#stratified sampling with the {rsample} package\neel_split <- initial_split(data = eel_model, prop = 0.7, strata = angaus)\n\neel_train <- training(eel_split)\neel_test <- testing(eel_split)\n\n# 10-fold cross validation, stratified by our outcome variable, angaus \ncv_folds <- eel_train |> \n  vfold_cv(v=10, strata = angaus)"
  },
  {
    "objectID": "posts/boosted_eels/index.html#preprocess",
    "href": "posts/boosted_eels/index.html#preprocess",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Preprocess",
    "text": "Preprocess\nNext, we create a recipe to prepape the data for the XGBoost model. For this analysis, we are interested in predicting the binary outcome variable angaus, which indicates presence or absence of the eel species Anguilla australis.\n\n\ncode\n# create a recipe \nboost_rec <- recipe(angaus ~., data = eel_train) |> \n  step_normalize(all_numeric()) |> \n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) |> \n  prep()\n\n# bake to check recipe\nbaked_train <- bake(boost_rec, eel_train) \nbaked_test <- bake(boost_rec, eel_test)"
  },
  {
    "objectID": "posts/boosted_eels/index.html#tuning-xgboost",
    "href": "posts/boosted_eels/index.html#tuning-xgboost",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Tuning XGBoost",
    "text": "Tuning XGBoost\n\nTune Learning Rate\nTo begin, we perform tuning on just the learn_rate parameter.\nWe create a model specification using {xbgoost} for the estimation, specifying only the learn_rate parameter for tuning.\n\n\ncode\n#create a model for specification\nlearn_spec <- boost_tree(learn_rate = tune()) |> \n  set_engine('xgboost') |> \n  set_mode('classification') \n\n\nNext, we build a grid to tune our model by using a range of learning rate parameter values.\n\n\ncode\n#set a grid to tune hyperparameter values\nlearn_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n\nThen, we define a new workflow and tune the model using the learning rate grid.\n\n\ncode\n#define a new workflow \nlearn_wf <- workflow() |> \n  add_model(learn_spec) |> \n  add_recipe(boost_rec)\n\ndoParallel::registerDoParallel()\nset.seed(123)\n\n#tune the model\nlearn_tune <- learn_wf |> \n  tune_grid(cv_folds, grid=learn_grid) \n\n#show the performance of the best models \nshow_best(learn_tune, metric = 'roc_auc') |> \n  tab_df(title = 'Table 2',\n         digits = 4,\n         footnote = 'Top performing models and their associated estimates for various learning rate parameter values.',\n         show.footnote=TRUE)\n\n\n\n\nTable 2\n\nlearn_rate\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n0.2690\nroc_auc\nbinary\n0.8524\n10\n0.0143\nPreprocessor1_Model27\n\n\n0.1862\nroc_auc\nbinary\n0.8451\n10\n0.0086\nPreprocessor1_Model19\n\n\n0.2586\nroc_auc\nbinary\n0.8444\n10\n0.0120\nPreprocessor1_Model26\n\n\n0.2380\nroc_auc\nbinary\n0.8442\n10\n0.0128\nPreprocessor1_Model24\n\n\n0.1759\nroc_auc\nbinary\n0.8434\n10\n0.0129\nPreprocessor1_Model18\n\n\nTop performing models and their associated estimates for various learning rate parameter values.\n\n\n\n\n\ncode\n#save the best model parameters to be used in future tuning\nbest_learnrate <- as.numeric(show_best(learn_tune, metric = 'roc_auc')[1,1])"
  },
  {
    "objectID": "posts/boosted_eels/index.html#tune-tree-parameters",
    "href": "posts/boosted_eels/index.html#tune-tree-parameters",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Tune Tree Parameters",
    "text": "Tune Tree Parameters\nFollowing the tuning of the learning rate parameter, we create a new specification with a set optimized learning rate from our previous optimization. Now, we shift our focus on tuning the tree parameters.\n\n\ncode\n#create a new specificatin with set learning rate \ntree_spec <- boost_tree(learn_rate = best_learnrate,\n                         min_n = tune(),\n                         tree_depth = tune(),\n                         loss_reduction = tune(),\n                         trees = 3000) |> \n  set_mode('classification') |> \n  set_engine('xgboost')\n\n\nAgain, we create a tuning grid, this time utilizing grid_max_entropy() to get a representative sampling of the parameter space.\n\n\ncode\n#specify parameters for the tuning grid \ntree_params <- dials::parameters(min_n(),\n                           tree_depth(),\n                           loss_reduction())\n\n#set the tuning grid \ntree_grid <- grid_max_entropy(tree_params, size = 20)\n\n#define a new workflow \ntree_wf <- workflow() |> \n  add_model(tree_spec) |> \n  add_recipe(boost_rec)\n\nset.seed(123)\ndoParallel::registerDoParallel()\n\n#tune the model\ntree_tuned <- tree_wf |> \n  tune_grid(cv_folds, grid = tree_grid)\n\n#show the performance of the best models \nshow_best(tree_tuned, metric = 'roc_auc') |> \n  tab_df(title = 'Table 3',\n         digits = 4,\n         footnote = 'Top performing models and their associated estimates for various tree parameter values.',\n         show.footnote = TRUE)\n\n\n\n\nTable 3\n\nmin_n\ntree_depth\nloss_reduction\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n3\n4\n0.0010\nroc_auc\nbinary\n0.8572\n10\n0.0154\nPreprocessor1_Model12\n\n\n6\n11\n0.0000\nroc_auc\nbinary\n0.8544\n10\n0.0150\nPreprocessor1_Model10\n\n\n4\n2\n0.0000\nroc_auc\nbinary\n0.8528\n10\n0.0143\nPreprocessor1_Model14\n\n\n7\n9\n4.9468\nroc_auc\nbinary\n0.8492\n10\n0.0130\nPreprocessor1_Model04\n\n\n4\n15\n0.0000\nroc_auc\nbinary\n0.8472\n10\n0.0141\nPreprocessor1_Model11\n\n\nTop performing models and their associated estimates for various tree parameter values.\n\n\n\n\n\ncode\n#save the best model parameters to be used for future tuning\nbest_minn <- as.numeric(show_best(tree_tuned, metric = 'roc_auc')[1,1])\n\nbest_treedepth <- as.numeric(show_best(tree_tuned, metric = 'roc_auc')[1,2])\n\nbest_lossreduction <- as.numeric(show_best(tree_tuned, metric = 'roc_auc')[1,3])"
  },
  {
    "objectID": "posts/boosted_eels/index.html#tune-stochastic-parameters",
    "href": "posts/boosted_eels/index.html#tune-stochastic-parameters",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Tune Stochastic Parameters",
    "text": "Tune Stochastic Parameters\nWe create one final specification, setting the learn rate and tree parameters to their optimal values defined in previous tuning iterations. Our final tuning is of the stochastic parameters.\n\n\ncode\n#create a new specificatin with set learning rate and tree parameters\nstoch_tune <- boost_tree(learn_rate = best_learnrate,\n                         min_n = best_minn,\n                         tree_depth = best_treedepth,\n                         loss_reduction = best_lossreduction,\n                         trees = 3000,\n                         mtry = tune(), \n                         sample_size = tune()) |> \n  set_mode('classification') |> \n  set_engine('xgboost')\n\n\nWe set up a tuning grid, again utilizing grid_max_entropy().\n\n\ncode\n#set parameters for the tuning grid \nstoch_params <- parameters(finalize(mtry(), eel_train),\n                          sample_size = sample_prop())\n\n#set the tuning grid \nstoch_grid <- grid_max_entropy(stoch_params, size = 20)\n\n#define a new workflow \nstoch_wf <- workflow() |> \n  add_model(stoch_tune) |> \n  add_recipe(boost_rec) \n\nset.seed(123)\ndoParallel::registerDoParallel()\n\n#tune the model \nstoch_tuned <- stoch_wf |> \n  tune_grid(cv_folds, grid = stoch_grid)\n\n#show the performance of the best models \nshow_best(stoch_tuned, metric = 'roc_auc') |> \n  tab_df(title = 'Table 4',\n         digits = 4,\n         footnote = 'Top performing models and their associated estimates for various stochiastic parameter values.',\n         show.footnote = TRUE)\n\n\n\n\nTable 4\n\nmtry\nsample_size\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n3\n0.9907\nroc_auc\nbinary\n0.8618\n10\n0.0136\nPreprocessor1_Model04\n\n\n8\n0.9930\nroc_auc\nbinary\n0.8586\n10\n0.0129\nPreprocessor1_Model19\n\n\n8\n0.7407\nroc_auc\nbinary\n0.8574\n10\n0.0151\nPreprocessor1_Model18\n\n\n2\n0.8529\nroc_auc\nbinary\n0.8559\n10\n0.0128\nPreprocessor1_Model10\n\n\n10\n0.8646\nroc_auc\nbinary\n0.8545\n10\n0.0137\nPreprocessor1_Model06\n\n\nTop performing models and their associated estimates for various stochiastic parameter values.\n\n\n\n\n\ncode\n#save the best model parameters to be used for model finalization\nbest_mtry <- as.numeric(show_best(stoch_tuned, metric = 'roc_auc')[1,1])\nbest_samplesize <- as.numeric(show_best(stoch_tuned, metric = 'roc_auc')[1,2])\n\n\nNow that we have tuned all of our relevant hyperparameters, we assemble a final workflow and do a final fit.\n\n\ncode\n#create a final specification with all set optimized parameters \nfinal_model <-  boost_tree(learn_rate = best_learnrate,\n                         min_n = best_minn,\n                         tree_depth = best_treedepth,\n                         loss_reduction = best_lossreduction,\n                         trees = 3000,\n                         mtry = best_mtry, \n                         sample_size = best_samplesize) |> \n  set_mode('classification') |> \n  set_engine('xgboost') \n\n#define a final workflow \nfinal_wf <- workflow() |> \n  add_model(final_model) |> \n  add_recipe(boost_rec) \n\n#fit training data on final wf\nfinal_fit <- final_wf |> \n  fit(eel_train)\n\nfinal_eel_fit <- last_fit(final_model, angaus~., eel_split)\n\nfinal_pred <- as.data.frame(final_eel_fit$.predictions)\n\ntab_df(head(final_pred),\n       title = 'Table 5',\n       digits = 4,\n       footnote = 'Predictions of Angaus presence on test data.',\n       show.footnote = TRUE)\n\n\n\n\nTable 5\n\n.pred_0\n.pred_1\n.row\n.pred_class\nangaus\n.config\n\n\n0.9999\n0.0001\n1\n0\n0\nPreprocessor1_Model1\n\n\n0.0085\n0.9915\n2\n1\n1\nPreprocessor1_Model1\n\n\n0.9810\n0.0190\n3\n0\n0\nPreprocessor1_Model1\n\n\n0.9468\n0.0532\n4\n0\n0\nPreprocessor1_Model1\n\n\n0.9995\n0.0005\n9\n0\n0\nPreprocessor1_Model1\n\n\n0.2945\n0.7055\n10\n1\n1\nPreprocessor1_Model1\n\n\nPredictions of Angaus presence on test data.\n\n\n\n\n\n\n\ncode\n#bind predictions and original data \neel_test_bind <- cbind(eel_test, final_eel_fit$.predictions)\n\n#remove duplicate column\neel_test_bind <- eel_test_bind[,-1]\n\n#compute a confusion matrix\nconfusion_matrix <- eel_test_bind |> \n  yardstick::conf_mat(truth = angaus, estimate = .pred_class)\n\nautoplot(confusion_matrix, type = \"heatmap\") +\n  scale_fill_gradient(low = \"#C7E9FB\", high = \"#084594\") +\n  theme(axis.text.x = element_text(size = 12),\n        axis.text.y = element_text(size = 12),\n        axis.title = element_text(size = 14),\n        panel.background = element_rect(fill = \"#F8F8F8\"),\n        plot.background = element_rect(fill = \"#F8F8F8\")) +\n  labs(title = \"Figure 2: Confusion matrix of predictions on test data.\")\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\ncode\n# store accuracy metrics \nfinal_metrics <- final_eel_fit$.metrics\n\ntab_df(final_metrics,\n       title = 'Table 6',\n       digits = 4,\n       footnote = 'Accuracy and Area Under the Receiver Operator Curve (ROC) of the final fit.',\n       show.footnote = TRUE)\n\n\n\n\nTable 6\n\n.metric\n.estimator\n.estimate\n.config\n\n\naccuracy\nbinary\n0.81063122923588\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.819193989071038\nPreprocessor1_Model1\n\n\nAccuracy and Area Under the Receiver Operator Curve (ROC) of the final fit.\n\n\n\n\n\nThe final model has an accuracy of 0.80. The ROC area under the curve is 0.82."
  },
  {
    "objectID": "posts/boosted_eels/index.html#fit-model-evaluation-data-and-compare-performance",
    "href": "posts/boosted_eels/index.html#fit-model-evaluation-data-and-compare-performance",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Fit model evaluation data and compare performance",
    "text": "Fit model evaluation data and compare performance\n\n\ncode\n#read in evaluation data \neval_data <- read_csv('eel.eval.data.csv', show_col_types = FALSE) |> \n  janitor::clean_names() |> \n  rename(angaus = angaus_obs)\n\n# convert presence of angaus, downstream obstructions, and fishing methods to factors\neval_data$angaus <- as.factor(eval_data$angaus)\neval_data$ds_dam <- as.factor(eval_data$ds_dam)\neval_data$method <- as.factor(eval_data$method)\n\n#fit final model to big dataset\n#class predictions\neval_classpred <- final_fit |> \n  predict(eval_data)\n\n#probability predictions\neval_probpred <- final_fit |> \n  predict(eval_data, type = 'prob')\n\neval_df <- cbind(eval_classpred, eval_probpred, eval_data)\n\n#accuracy measure\naccuracy <- accuracy(eval_df, truth = angaus, estimate = .pred_class)\n#roc_auc measure \nroc <- roc_auc(eval_df, truth = angaus, estimate = .pred_0)\n\nmetrics <- rbind(accuracy, roc)\ntab_df(metrics, \n       title = 'Table 7',\n       digits = 4,\n       footnote = 'Accuracy and Area Under the Receiver Operator Curve (ROC) of the model fit to evaluation data.',\n       show.footnote = TRUE)\n\n\n\n\nTable 7\n\n.metric\n.estimator\n.estimate\n\n\naccuracy\nbinary\n0.8200\n\n\nroc_auc\nbinary\n0.8476\n\n\nAccuracy and Area Under the Receiver Operator Curve (ROC) of the model fit to evaluation data.\n\n\n\n\n\n\nHow does this model perform on the evaluation data?\nThe final model, fit to the evaluation data, has an accuracy of 0.82, and the ROC area under the curve is 0.85. For comparison, the model produced by Edith et al. had a ROC area under the curve of 0.858."
  },
  {
    "objectID": "posts/boosted_eels/index.html#references",
    "href": "posts/boosted_eels/index.html#references",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "References",
    "text": "References\n[1] Elith, J., Leathwick, J.R. and Hastie, T. (2008), A working guide to boosted regression trees. Journal of Animal Ecology, 77: 802-813. https://doi.org/10.1111/j.1365-2656.2008.01390.x"
>>>>>>> parent of 93be2ae (finalizing eel modeling)
  }
]