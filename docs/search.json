[
  {
    "objectID": "workshops/index.html",
    "href": "workshops/index.html",
    "title": "Intro to R: Data Wrangling",
    "section": "",
    "text": "Illustration from Hadley Wickham’s 2019 talk, The Joy of Functional Programming\n\n\n\n\n\n Instructional Documentation\n\n\n Source Code\n\n\n\n\nOverview\nThis workshop was developed for ES40: Critical Thinking and Evidence Based Reasoning at the University of California, Santa Barbara.\n\n\nAbstract\nR is a programming language that enables us to turn data into understanding. In this workshop, we’ll introduce the {dplyr} package of the {tidyverse}, which houses many functions that make data easier to work with, as an introduction to R and its applications for environmental data."
  },
  {
    "objectID": "posts/tfp_analysis/index.html",
    "href": "posts/tfp_analysis/index.html",
    "title": "Agriculture Total Factor Productivity Growth in Global Income Classes",
    "section": "",
    "text": "The agricultural sector faces opposing pressures of sustaining a growing population while minimizing its unfavorable outcomes on finite environmental resources1. In an effort to simultaneously move towards these goals, countries around the world have prioritized agricultural productivity. One of the most informative measures of agricultural productivity is total factor productivity (TFP). TFP compares gross outputs of crop, animal and aquaculture products to inputs of land, labor, capital and material resources utilized in farm production2. As gross output increases at a faster rate than total inputs, total factor production improves, which eases tensions on environmental resources and food security, and boosts economic growth3. TFP is expressed generally by the equation: \\[TFP=Y/X\\] where Y represents gross output and X represents total inputs.\nTFP is an important measure for informing policy priorities for agricultural productivity. These policies include investments in research and development, incentivizing economic reforms for farmers, rural education and extension, and improvments in infrastructure4. Understanding the effects of individual inputs on TFP can direct decision making as it relates to resource allocation for these policy investments.\n\n\n\n\n\nThis analysis will regress global TFP indices on inputs of land, labor, capital, and materials to examine the effects of these inputs on gross outputs. This regression can be utilized to maximize TFP growth rates by differentiating efficiency levels of individual inputs as they relate to gross productivity, which can direct resource allocation to technological improvements of inefficient input systems.\nAdditionally, it will examine TFP growth rates for country groupings of income class (defined by the World Bank) by testing for mean differences. It will also forecast TFP growth rates for years 2020-2030 at a global scale and for income classes by employing automated autoregressive moving average (ARIMA) models. Understanding nuances in TFP growth for varying income scales can be useful in further research to refine regressions that direct policy prioritization.\nAll relevant analysis outputs are included in the Analysis section - for detailed code concerning model checking, reference the Model Testing and Supporting Figures section."
  },
  {
    "objectID": "posts/tfp_analysis/index.html#footnotes",
    "href": "posts/tfp_analysis/index.html#footnotes",
    "title": "Agriculture Total Factor Productivity Growth in Global Income Classes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNetwork on Agricultural Total Factor Productivity and the Environment. Oecd.org. Accessed December 4, 2022. https://www.oecd.org/agriculture/topics/network-agricultural-productivity-and-environment/↩︎\nFuglie K, Jelliffe J, Morgan S. Documentation and methods. Usda.gov. Published October 7, 2022. Accessed December 4, 2022. https://www.ers.usda.gov/data-products/international-agricultural-productivity/documentation-and-methods/↩︎\nFuglie K, Jelliffe J, Morgan S. Documentation and methods. Usda.gov. Published October 7, 2022. Accessed December 4, 2022. https://www.ers.usda.gov/data-products/international-agricultural-productivity/documentation-and-methods/↩︎\nFuglie K, Wang SL. New Evidence Points to Robust But Uneven Productivity Growth in Global Agriculture. Usda.gov. Published September 20, 2012. Accessed December 4, 2022. https://www.ers.usda.gov/amber-waves/2012/september/global-agriculture/↩︎\nFuglie K, Jelliffe J, Morgan S. International agricultural productivity. Usda.gov. Published October 7, 2022. Accessed December 4, 2022. https://www.ers.usda.gov/data-products/international-agricultural-productivity/↩︎\nGriffith B. Middle-Income Trap. In: Frontiers in Development Policy. The World Bank; 2011:39-43.↩︎\nFuglie K, Wang SL. New Evidence Points to Robust But Uneven Productivity Growth in Global Agriculture. Usda.gov. Published September 20, 2012. Accessed December 4, 2022. https://www.ers.usda.gov/amber-waves/2012/september/global-agriculture/↩︎\nNetwork on Agricultural Total Factor Productivity and the Environment. Oecd.org. Accessed December 4, 2022. https://www.oecd.org/agriculture/topics/network-agricultural-productivity-and-environment/↩︎"
  },
  {
    "objectID": "posts/boosted_eels/index.html",
    "href": "posts/boosted_eels/index.html",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "",
    "text": "In this post, I perform a case study example of species distribution modeling in a reproduction of the work done by Edith et al. 2008 [1]. For this case study, we model the short-finned eel (Anguilla australis) species using Boosted Regression Trees. This analysis is derived from an assignment for EDS232: Machine Learning in Environmental Science, as part of the curriculum for UCSB’s Master’s of Environmental Data Science program.\nBoosted regression trees, also known as gradient boosting machines, are machine learning techniques that combine concepts of regression trees (relating responses to predictors using recursive binary splits) and iterative boosting (an adaptive method for combining simple models to improve predictive performance). In this case study, decision trees are built sequentially using extreme gradient boosting to classify the presence or absence of short-finned eel in a given location with covariates including temperature, slope, rainy days, etc. The work provided by Edith et al. utilizes the {gbm} package in R, while this analysis follows a {tidymodels} approach in R."
  },
  {
    "objectID": "posts/boosted_eels/index.html#description",
    "href": "posts/boosted_eels/index.html#description",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "",
    "text": "In this post, I perform a case study example of species distribution modeling in a reproduction of the work done by Edith et al. 2008 [1]. For this case study, we model the short-finned eel (Anguilla australis) species using Boosted Regression Trees. This analysis is derived from an assignment for EDS232: Machine Learning in Environmental Science, as part of the curriculum for UCSB’s Master’s of Environmental Data Science program.\nBoosted regression trees, also known as gradient boosting machines, are machine learning techniques that combine concepts of regression trees (relating responses to predictors using recursive binary splits) and iterative boosting (an adaptive method for combining simple models to improve predictive performance). In this case study, decision trees are built sequentially using extreme gradient boosting to classify the presence or absence of short-finned eel in a given location with covariates including temperature, slope, rainy days, etc. The work provided by Edith et al. utilizes the {gbm} package in R, while this analysis follows a {tidymodels} approach in R."
  },
  {
    "objectID": "posts/boosted_eels/index.html#data",
    "href": "posts/boosted_eels/index.html#data",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Data",
    "text": "Data\nOur data for this analysis, “eel.model.data.csv” were retrived from the supplemental information of Edith et al. 2008. These data include the following variables:\n\n\n\nFigure 1: Table 1. from Elith et al. 2008 displaying the variables included in the analysis.\n\n\n\n\ncode\n# load required libraries\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(glmnet)\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(sjPlot)\nlibrary(pROC)\nlibrary(RColorBrewer)\n\n\neel_data &lt;- eel_data_raw |&gt; \n  janitor::clean_names() \n\n# remove site number from modeling data\neel_model &lt;- eel_data[,-1]\n\n# convert presence of angaus, downstream obstructions, and fishing methods to factors\neel_model$angaus &lt;- as.factor(eel_model$angaus)\neel_model$ds_dam &lt;- as.factor(eel_model$ds_dam)\neel_model$method &lt;- as.factor(eel_model$method)\n\ntab_df(eel_data[1:5,],\n       title = \"Table. 1\")\n\n\n\nTable. 1\n\n\nsite\nangaus\nseg_sum_t\nseg_t_seas\nseg_low_flow\nds_dist\nds_max_slope\nus_avg_t\nus_rain_days\nus_slope\nus_native\nds_dam\nmethod\nloc_sed\n\n\n1\n0\n16.00\n-0.10\n1.04\n50.20\n0.57\n0.09\n2.47\n9.80\n0.81\n0\nelectric\n4.80\n\n\n2\n1\n18.70\n1.51\n1.00\n132.53\n1.15\n0.20\n1.15\n8.30\n0.34\n0\nelectric\n2.00\n\n\n3\n0\n18.30\n0.37\n1.00\n107.44\n0.57\n0.49\n0.85\n0.40\n0.00\n0\nspo\n1.00\n\n\n4\n0\n16.70\n-3.80\n1.00\n166.82\n1.72\n0.90\n0.21\n0.40\n0.22\n1\nelectric\n4.00\n\n\n5\n1\n17.20\n0.33\n1.00\n3.95\n1.15\n-1.20\n1.98\n21.90\n0.96\n0\nelectric\n4.70"
  },
  {
    "objectID": "posts/boosted_eels/index.html#split-and-resample",
    "href": "posts/boosted_eels/index.html#split-and-resample",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Split and Resample",
    "text": "Split and Resample\nWe split the above data into a training and test set, stratifying by the outcome score (angaus) to maintain class balance, improve generalization, and ensure the reliability of evaluation in modeling performances. We then use a 10-fold cross validation to resample the training set, also stratified by outcome score.\n\n\ncode\n# set a seed for reproducibility\nset.seed(123)\n\n#stratified sampling with the {rsample} package\neel_split &lt;- initial_split(data = eel_model, prop = 0.7, strata = angaus)\n\neel_train &lt;- training(eel_split)\neel_test &lt;- testing(eel_split)\n\n# 10-fold cross validation, stratified by our outcome variable, angaus \ncv_folds &lt;- eel_train |&gt; \n  vfold_cv(v=10, strata = angaus)"
  },
  {
    "objectID": "posts/boosted_eels/index.html#preprocess",
    "href": "posts/boosted_eels/index.html#preprocess",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Preprocess",
    "text": "Preprocess\nNext, we create a recipe to prepape the data for the XGBoost model. For this analysis, we are interested in predicting the binary outcome variable angaus, which indicates presence or absence of the eel species Anguilla australis.\n\n\ncode\n# create a recipe \nboost_rec &lt;- recipe(angaus ~., data = eel_train) |&gt; \n  step_normalize(all_numeric()) |&gt; \n  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) |&gt; \n  prep()\n\n# bake to check recipe\nbaked_train &lt;- bake(boost_rec, eel_train) \nbaked_test &lt;- bake(boost_rec, eel_test)"
  },
  {
    "objectID": "posts/boosted_eels/index.html#tuning-xgboost",
    "href": "posts/boosted_eels/index.html#tuning-xgboost",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Tuning XGBoost",
    "text": "Tuning XGBoost\n\nTune Learning Rate\nTo begin, we perform tuning on just the learn_rate parameter.\nWe create a model specification using {xbgoost} for the estimation, specifying only the learn_rate parameter for tuning.\n\n\ncode\n#create a model for specification\nlearn_spec &lt;- boost_tree(learn_rate = tune()) |&gt; \n  set_engine('xgboost') |&gt; \n  set_mode('classification') \n\n\nNext, we build a grid to tune our model by using a range of learning rate parameter values.\n\n\ncode\n#set a grid to tune hyperparameter values\nlearn_grid &lt;- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))\n\n\nThen, we define a new workflow and tune the model using the learning rate grid.\n\n\ncode\n#define a new workflow \nlearn_wf &lt;- workflow() |&gt; \n  add_model(learn_spec) |&gt; \n  add_recipe(boost_rec)\n\ndoParallel::registerDoParallel()\nset.seed(123)\n\n#tune the model\nlearn_tune &lt;- learn_wf |&gt; \n  tune_grid(cv_folds, grid=learn_grid) \n\n#show the performance of the best models \nshow_best(learn_tune, metric = 'roc_auc') |&gt; \n  tab_df(title = 'Table 2',\n         digits = 4,\n         footnote = 'Top performing models and their associated estimates for various learning rate parameter values.',\n         show.footnote=TRUE)\n\n\n\nTable 2\n\n\nlearn_rate\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n0.2690\nroc_auc\nbinary\n0.8524\n10\n0.0143\nPreprocessor1_Model27\n\n\n\n0.1862\nroc_auc\nbinary\n0.8451\n10\n0.0086\nPreprocessor1_Model19\n\n\n\n0.2586\nroc_auc\nbinary\n0.8444\n10\n0.0120\nPreprocessor1_Model26\n\n\n\n0.2380\nroc_auc\nbinary\n0.8442\n10\n0.0128\nPreprocessor1_Model24\n\n\n\n0.1759\nroc_auc\nbinary\n0.8434\n10\n0.0129\nPreprocessor1_Model18\n\n\n\nTop performing models and their associated estimates for various learning rate parameter values.\n\n\n\n\n\n\n\ncode\n#save the best model parameters to be used in future tuning\nbest_learnrate &lt;- as.numeric(show_best(learn_tune, metric = 'roc_auc')[1,1])"
  },
  {
    "objectID": "posts/boosted_eels/index.html#tune-tree-parameters",
    "href": "posts/boosted_eels/index.html#tune-tree-parameters",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Tune Tree Parameters",
    "text": "Tune Tree Parameters\nFollowing the tuning of the learning rate parameter, we create a new specification with a set optimized learning rate from our previous optimization. Now, we shift our focus on tuning the tree parameters.\n\n\ncode\n#create a new specificatin with set learning rate \ntree_spec &lt;- boost_tree(learn_rate = best_learnrate,\n                         min_n = tune(),\n                         tree_depth = tune(),\n                         loss_reduction = tune(),\n                         trees = 3000) |&gt; \n  set_mode('classification') |&gt; \n  set_engine('xgboost')\n\n\nAgain, we create a tuning grid, this time utilizing grid_max_entropy() to get a representative sampling of the parameter space.\n\n\ncode\n#specify parameters for the tuning grid \ntree_params &lt;- dials::parameters(min_n(),\n                           tree_depth(),\n                           loss_reduction())\n\n#set the tuning grid \ntree_grid &lt;- grid_max_entropy(tree_params, size = 20)\n\n#define a new workflow \ntree_wf &lt;- workflow() |&gt; \n  add_model(tree_spec) |&gt; \n  add_recipe(boost_rec)\n\nset.seed(123)\ndoParallel::registerDoParallel()\n\n#tune the model\ntree_tuned &lt;- tree_wf |&gt; \n  tune_grid(cv_folds, grid = tree_grid)\n\n#show the performance of the best models \nshow_best(tree_tuned, metric = 'roc_auc') |&gt; \n  tab_df(title = 'Table 3',\n         digits = 4,\n         footnote = 'Top performing models and their associated estimates for various tree parameter values.',\n         show.footnote = TRUE)\n\n\n\nTable 3\n\n\nmin_n\ntree_depth\nloss_reduction\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n3\n4\n0.0010\nroc_auc\nbinary\n0.8572\n10\n0.0154\nPreprocessor1_Model12\n\n\n\n6\n11\n0.0000\nroc_auc\nbinary\n0.8544\n10\n0.0150\nPreprocessor1_Model10\n\n\n\n4\n2\n0.0000\nroc_auc\nbinary\n0.8528\n10\n0.0143\nPreprocessor1_Model14\n\n\n\n7\n9\n4.9468\nroc_auc\nbinary\n0.8492\n10\n0.0130\nPreprocessor1_Model04\n\n\n\n4\n15\n0.0000\nroc_auc\nbinary\n0.8472\n10\n0.0141\nPreprocessor1_Model11\n\n\n\nTop performing models and their associated estimates for various tree parameter values.\n\n\n\n\n\n\n\ncode\n#save the best model parameters to be used for future tuning\nbest_minn &lt;- as.numeric(show_best(tree_tuned, metric = 'roc_auc')[1,1])\n\nbest_treedepth &lt;- as.numeric(show_best(tree_tuned, metric = 'roc_auc')[1,2])\n\nbest_lossreduction &lt;- as.numeric(show_best(tree_tuned, metric = 'roc_auc')[1,3])"
  },
  {
    "objectID": "posts/boosted_eels/index.html#tune-stochastic-parameters",
    "href": "posts/boosted_eels/index.html#tune-stochastic-parameters",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Tune Stochastic Parameters",
    "text": "Tune Stochastic Parameters\nWe create one final specification, setting the learn rate and tree parameters to their optimal values defined in previous tuning iterations. Our final tuning is of the stochastic parameters.\n\n\ncode\n#create a new specificatin with set learning rate and tree parameters\nstoch_tune &lt;- boost_tree(learn_rate = best_learnrate,\n                         min_n = best_minn,\n                         tree_depth = best_treedepth,\n                         loss_reduction = best_lossreduction,\n                         trees = 3000,\n                         mtry = tune(), \n                         sample_size = tune()) |&gt; \n  set_mode('classification') |&gt; \n  set_engine('xgboost')\n\n\nWe set up a tuning grid, again utilizing grid_max_entropy().\n\n\ncode\n#set parameters for the tuning grid \nstoch_params &lt;- parameters(finalize(mtry(), eel_train),\n                          sample_size = sample_prop())\n\n#set the tuning grid \nstoch_grid &lt;- grid_max_entropy(stoch_params, size = 20)\n\n#define a new workflow \nstoch_wf &lt;- workflow() |&gt; \n  add_model(stoch_tune) |&gt; \n  add_recipe(boost_rec) \n\nset.seed(123)\ndoParallel::registerDoParallel()\n\n#tune the model \nstoch_tuned &lt;- stoch_wf |&gt; \n  tune_grid(cv_folds, grid = stoch_grid)\n\n#show the performance of the best models \nshow_best(stoch_tuned, metric = 'roc_auc') |&gt; \n  tab_df(title = 'Table 4',\n         digits = 4,\n         footnote = 'Top performing models and their associated estimates for various stochiastic parameter values.',\n         show.footnote = TRUE)\n\n\n\nTable 4\n\n\nmtry\nsample_size\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n3\n0.9907\nroc_auc\nbinary\n0.8618\n10\n0.0136\nPreprocessor1_Model04\n\n\n\n8\n0.9930\nroc_auc\nbinary\n0.8586\n10\n0.0129\nPreprocessor1_Model19\n\n\n\n8\n0.7407\nroc_auc\nbinary\n0.8574\n10\n0.0151\nPreprocessor1_Model18\n\n\n\n2\n0.8529\nroc_auc\nbinary\n0.8559\n10\n0.0128\nPreprocessor1_Model10\n\n\n\n10\n0.8646\nroc_auc\nbinary\n0.8545\n10\n0.0137\nPreprocessor1_Model06\n\n\n\nTop performing models and their associated estimates for various stochiastic parameter values.\n\n\n\n\n\n\n\ncode\n#save the best model parameters to be used for model finalization\nbest_mtry &lt;- as.numeric(show_best(stoch_tuned, metric = 'roc_auc')[1,1])\nbest_samplesize &lt;- as.numeric(show_best(stoch_tuned, metric = 'roc_auc')[1,2])\n\n\nNow that we have tuned all of our relevant hyperparameters, we assemble a final workflow and do a final fit.\n\n\ncode\n#create a final specification with all set optimized parameters \nfinal_model &lt;-  boost_tree(learn_rate = best_learnrate,\n                         min_n = best_minn,\n                         tree_depth = best_treedepth,\n                         loss_reduction = best_lossreduction,\n                         trees = 3000,\n                         mtry = best_mtry, \n                         sample_size = best_samplesize) |&gt; \n  set_mode('classification') |&gt; \n  set_engine('xgboost') \n\n#define a final workflow \nfinal_wf &lt;- workflow() |&gt; \n  add_model(final_model) |&gt; \n  add_recipe(boost_rec) \n\n#fit training data on final wf\nfinal_fit &lt;- final_wf |&gt; \n  fit(eel_train)\n\nfinal_eel_fit &lt;- last_fit(final_model, angaus~., eel_split)\n\nfinal_pred &lt;- as.data.frame(final_eel_fit$.predictions)\n\ntab_df(head(final_pred),\n       title = 'Table 5',\n       digits = 4,\n       footnote = 'Predictions of Angaus presence on test data.',\n       show.footnote = TRUE)\n\n\n\nTable 5\n\n\n.pred_0\n.pred_1\n.row\n.pred_class\nangaus\n.config\n\n\n\n0.9999\n0.0001\n1\n0\n0\nPreprocessor1_Model1\n\n\n\n0.0085\n0.9915\n2\n1\n1\nPreprocessor1_Model1\n\n\n\n0.9810\n0.0190\n3\n0\n0\nPreprocessor1_Model1\n\n\n\n0.9468\n0.0532\n4\n0\n0\nPreprocessor1_Model1\n\n\n\n0.9995\n0.0005\n9\n0\n0\nPreprocessor1_Model1\n\n\n\n0.2945\n0.7055\n10\n1\n1\nPreprocessor1_Model1\n\n\n\nPredictions of Angaus presence on test data.\n\n\n\n\n\n\n\n\n\ncode\n#bind predictions and original data \neel_test_bind &lt;- cbind(eel_test, final_eel_fit$.predictions)\n\n#remove duplicate column\neel_test_bind &lt;- eel_test_bind[,-1]\n\n#compute a confusion matrix\nconfusion_matrix &lt;- eel_test_bind |&gt; \n  yardstick::conf_mat(truth = angaus, estimate = .pred_class)\n\nautoplot(confusion_matrix, type = \"heatmap\") +\n  scale_fill_gradient(low = \"#C7E9FB\", high = \"#084594\") +\n  theme(axis.text.x = element_text(size = 12),\n        axis.text.y = element_text(size = 12),\n        axis.title = element_text(size = 14),\n        panel.background = element_rect(fill = \"#F8F8F8\"),\n        plot.background = element_rect(fill = \"#F8F8F8\")) +\n  labs(title = \"Figure 2: Confusion matrix of predictions on test data.\")\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\ncode\n# store accuracy metrics \nfinal_metrics &lt;- final_eel_fit$.metrics\n\ntab_df(final_metrics,\n       title = 'Table 6',\n       digits = 4,\n       footnote = 'Accuracy and Area Under the Receiver Operator Curve (ROC) of the final fit.',\n       show.footnote = TRUE)\n\n\n\nTable 6\n\n\n.metric\n.estimator\n.estimate\n.config\n\n\n\naccuracy\nbinary\n0.81063122923588\nPreprocessor1_Model1\n\n\n\nroc_auc\nbinary\n0.819193989071038\nPreprocessor1_Model1\n\n\n\nAccuracy and Area Under the Receiver Operator Curve (ROC) of the final fit.\n\n\n\n\n\n\n\nThe final model has an accuracy of 0.80. The ROC area under the curve is 0.82."
  },
  {
    "objectID": "posts/boosted_eels/index.html#fit-model-evaluation-data-and-compare-performance",
    "href": "posts/boosted_eels/index.html#fit-model-evaluation-data-and-compare-performance",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "Fit model evaluation data and compare performance",
    "text": "Fit model evaluation data and compare performance\n\n\ncode\n#read in evaluation data \neval_data &lt;- read_csv('eel.eval.data.csv', show_col_types = FALSE) |&gt; \n  janitor::clean_names() \n\n# convert presence of angaus, downstream obstructions, and fishing methods to factors\neval_data$angaus &lt;- as.factor(eval_data$angaus_obs)\neval_data$ds_dam &lt;- as.factor(eval_data$ds_dam)\neval_data$method &lt;- as.factor(eval_data$method)\n\n#fit final model to big dataset\n#class predictions\neval_classpred &lt;- final_fit |&gt; \n  predict(eval_data)\n\n#probability predictions\neval_probpred &lt;- final_fit |&gt; \n  predict(eval_data, type = 'prob')\n\neval_df &lt;- cbind(eval_classpred, eval_probpred, eval_data)\n\n#accuracy measure\naccuracy &lt;- accuracy(eval_df, truth = angaus, estimate = .pred_class)\n#roc_auc measure \n#roc &lt;- yardstick::roc_auc(eval_df, truth = angaus, estimate = .pred_0)\n\n# metrics &lt;- rbind(accuracy, roc)\n# tab_df(metrics, \n#        title = 'Table 7',\n#        digits = 4,\n#        footnote = 'Accuracy and Area Under the Receiver Operator Curve (ROC) of the model fit to evaluation data.',\n#        show.footnote = TRUE)\n\n\n\nHow does this model perform on the evaluation data?\nThe final model, fit to the evaluation data, has an accuracy of 0.82, and the ROC area under the curve is 0.85. For comparison, the model produced by Edith et al. had a ROC area under the curve of 0.858."
  },
  {
    "objectID": "posts/boosted_eels/index.html#references",
    "href": "posts/boosted_eels/index.html#references",
    "title": "Eel Species Distribution Modeling Using Boosted Trees",
    "section": "References",
    "text": "References\n[1] Elith, J., Leathwick, J.R. and Hastie, T. (2008), A working guide to boosted regression trees. Journal of Animal Ecology, 77: 802-813. https://doi.org/10.1111/j.1365-2656.2008.01390.x"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Workshops",
    "section": "",
    "text": "Intro to R: Data Wrangling\n\n\na code-along workshop to learn basic examples of using quantitative data in environmental data science with the {tidyverse}\n\n\n\nGabrielle Smith\n\n\nApr 27, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gabrielle Smith",
    "section": "",
    "text": "I’m Gabrielle, and I am a recent graduate of the Bren School of Environmental Science & Management at the University of California, Santa Barbara with a Master’s in Environmental Data Science. I also earned a Statistics and Data Science B.S. from the University of California, Santa Barbara in 2022.\nI’m passionate about leveraging remote sensing techniques to address our environmental challenges and climate issues. If you’re curious about some of the projects I’ve worked on, you came to the right place!"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Gabrielle Smith",
    "section": "",
    "text": "I’m Gabrielle, and I am a recent graduate of the Bren School of Environmental Science & Management at the University of California, Santa Barbara with a Master’s in Environmental Data Science. I also earned a Statistics and Data Science B.S. from the University of California, Santa Barbara in 2022.\nI’m passionate about leveraging remote sensing techniques to address our environmental challenges and climate issues. If you’re curious about some of the projects I’ve worked on, you came to the right place!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Gabrielle Smith",
    "section": "Education",
    "text": "Education\n\nGraduate\nUniversity of California, Santa Barbara | Santa Barbara, CA (June 2023)\nMaster of Environmental Data Science\n\n\nUndergraduate\nUniversity of California, Santa Barbara | Santa Barbara, CA (March 2022)\nBachelor of Science in Statistics and Data Science"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I grew up in San Diego, California. I moved to Santa Barbara in 2018 to pursue a Bachelor’s of Science in Statistics and Data Science. During that time, I experimented, a lot, with different educational enthusiasms.\nThroughout my life, I have always had a passion for people and the world around us. This passion turned into a career aspiration after a transformative volunteer trip to Thailand with GIVE Volunteers in 2019.\n\n\n\n\n\n\n\n\nA couple of friends that I made during my time in Thailand!\n\nDuring my two week experience abroad, I had the opportunity to establish a functional permaculture agricultural system in a remote village in Mueang Kong. For context, permaculture is a holistic design philosophy that promotes sustainable and regenerative farming practices. It strives to maximize productivity while minimizing negative environmental impacts by emulating natural patterns and processes. To learn more about permaculture in agriculture, you can find additional information here.\n\n\n\n\n\n\n\nI was fascinated by this pursuit of harmony between people and the natural world, and it has since become the driving force behind both my personal and professional goals. Inspired by this experience, I decided to pursue a Master’s degree in Environmental Data Science at the University of California, Santa Barbara. This choice allowed me to merge my educational background with my aspiration to contribute to systems that promote the well-being of both people and the planet.\nSince then, I have honed a diverse skill set in data science, equipping me with the tools to employ data-driven decision making in addressing environmental challenges in a sustainable and equitable way. My aim is to apply these skills in future career endeavors, working towards a better world that cares for the health of all living beings."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Portfolio",
    "section": "",
    "text": "Agriculture Total Factor Productivity Growth in Global Income Classes\n\n\nA statistical analysis of agriculture total factor productivity indices on global and income class scales, employing methods of multiple linear regression, mean testing, and…\n\n\n\nGabrielle Smith\n\n\nDec 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEel Species Distribution Modeling Using Boosted Trees\n\n\nA case study of species distribution modeling, following a modeling project described by Edith et al. 2008.\n\n\n\nGabrielle Smith\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]